# Dropbox â†’ GCS â†’ Vertex AI Search & Multimodal Embeddings

Whole-file retrieval system that mirrors Dropbox to Google Cloud Storage,
indexes **images** via Vertex AI Vector Search (multimodal embeddings) and
**documents** via Vertex AI Search â€” queried exclusively through **cURL**.

---

## ğŸš€ Quick Start

**Infrastructure deployed?** â†’ See [TESTING.md](TESTING.md) for unblocking and testing.

**Starting fresh?** â†’ Follow [Setup](#setup-one-time) below.

**Current Status**: Infrastructure complete. **Testing blocked** â€” Dropbox permissions need fixing. See [TESTING.md](TESTING.md).

---

## Architecture

```
Dropbox (source of truth)
  â”‚
  â–¼
Cloud Run Job A  (daily sync)
  â”‚
  â–¼
GCS  gs://<PROJECT>-dropbox-mirror/
  â”œâ”€â”€ mirror/images/<file_id>          â”€â–º Cloud Run Job B â”€â–º Vertex AI Vector Search
  â”œâ”€â”€ mirror/docs/<file_id>            â”€â–º Vertex AI Search datastore (periodic import)
  â”œâ”€â”€ mirror/media/<file_id>           (stored only)
  â”œâ”€â”€ mirror/meta/<file_id>.json       (metadata sidecar)
  â””â”€â”€ mirror/state/
        â”œâ”€â”€ sync_state.json            (Dropbox cursor)
        â”œâ”€â”€ path_index.json            (path â†’ file_id reverse lookup)
        â””â”€â”€ embedding_state.json       (file_id â†’ embedded rev)

Retrieval: cURL only (no Python search API)
  â”œâ”€â”€ curl/query_vector_search.sh      text â†’ embedding â†’ findNeighbors (images)
  â”œâ”€â”€ curl/query_vertex_search.sh      text â†’ Discovery Engine search  (docs)
  â””â”€â”€ curl/combine_results.sh          both queries, merged JSON output
```

---

## Repo Structure

```
.
â”œâ”€â”€ shared/                            # Shared Python library
â”‚   â”œâ”€â”€ config.py                      # Env-var config
â”‚   â”œâ”€â”€ categories.py                  # Extension â†’ category mapping
â”‚   â”œâ”€â”€ gcs.py                         # GCS helper functions
â”‚   â””â”€â”€ dropbox_client.py              # Dropbox SDK wrapper (refresh-token)
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ sync_dropbox_to_gcs/           # Job A â€” Dropbox â†’ GCS mirror
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ embed_images_to_vector_search/ # Job B â€” image embeddings â†’ Vector Search
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ infra/                             # One-time GCP provisioning (gcloud)
â”‚   â”œâ”€â”€ variables.sh                   # â† EDIT THIS FIRST
â”‚   â”œâ”€â”€ 01_setup_gcp.sh
â”‚   â”œâ”€â”€ 02_create_bucket.sh
â”‚   â”œâ”€â”€ 03_create_vector_search.sh
â”‚   â”œâ”€â”€ 04_create_vertex_search.sh
â”‚   â”œâ”€â”€ 05_build_and_deploy_jobs.sh
â”‚   â”œâ”€â”€ 06_create_scheduler.sh
â”‚   â””â”€â”€ store_secrets.sh
â”‚
â””â”€â”€ curl/                               # cURL-only retrieval scripts
    â”œâ”€â”€ query_vector_search.sh
    â”œâ”€â”€ query_vertex_search.sh
    â””â”€â”€ combine_results.sh
```

---

## Prerequisites

| Requirement | Detail |
|---|---|
| Google Cloud project | Billing enabled |
| `gcloud` CLI | Authenticated (`gcloud auth login`) |
| Docker | For building Cloud Run Job images |
| Dropbox app | Scoped access, `files.metadata.read` + `files.content.read` |
| Dropbox refresh token | OAuth2 flow â†’ app key, app secret, refresh token |

---

## Environment Variables

All configuration is via environment variables (no `.env` files).

### Always required

| Variable | Description |
|---|---|
| `GCP_PROJECT_ID` | Google Cloud project ID |
| `GCS_BUCKET_NAME` | GCS bucket name |
| `DROPBOX_APP_KEY` | Dropbox app key |
| `DROPBOX_APP_SECRET` | Dropbox app secret |
| `DROPBOX_REFRESH_TOKEN` | Dropbox OAuth2 refresh token |

### Required after infra setup

| Variable | Description |
|---|---|
| `GCP_REGION` | Default: `us-central1` |
| `VECTOR_SEARCH_INDEX_ID` | From `03_create_vector_search.sh` output |
| `VECTOR_SEARCH_ENDPOINT_ID` | From `03_create_vector_search.sh` output |
| `VECTOR_SEARCH_DEPLOYED_INDEX_ID` | From `03_create_vector_search.sh` output |
| `VERTEX_SEARCH_DATASTORE_ID` | From `04_create_vertex_search.sh` output |
| `VERTEX_SEARCH_ENGINE_ID` | From `04_create_vertex_search.sh` output |

---

## Setup (One-Time)

### 1. Edit variables

```bash
vim infra/variables.sh   # set PROJECT_ID at minimum
```

### 2. Store Dropbox secrets

```bash
bash infra/store_secrets.sh <APP_KEY> <APP_SECRET> <REFRESH_TOKEN>
```

### 3. Run infra scripts in order

```bash
bash infra/01_setup_gcp.sh
bash infra/02_create_bucket.sh
bash infra/03_create_vector_search.sh    # â³ ~30-60 min (index + deploy)
bash infra/04_create_vertex_search.sh
bash infra/05_build_and_deploy_jobs.sh   # edit Vector Search IDs first
bash infra/06_create_scheduler.sh
```

Each script prints the resource IDs you need for the next step.

---

## Manual Execution

```bash
# Run sync (Dropbox â†’ GCS)
gcloud run jobs execute sync-dropbox-to-gcs --region=us-central1

# Run embedding (images â†’ Vector Search)
gcloud run jobs execute embed-images-to-vector-search --region=us-central1
```

Or locally (for development):

```bash
export GCP_PROJECT_ID=my-project
export GCS_BUCKET_NAME=my-project-dropbox-mirror
export DROPBOX_APP_KEY=xxx
export DROPBOX_APP_SECRET=xxx
export DROPBOX_REFRESH_TOKEN=xxx

python jobs/sync_dropbox_to_gcs/main.py

export VECTOR_SEARCH_INDEX_ID=123456
export VECTOR_SEARCH_ENDPOINT_ID=789012
export VECTOR_SEARCH_DEPLOYED_INDEX_ID=deployed_dropbox_images
python jobs/embed_images_to_vector_search/main.py
```

---

## Querying (cURL Only)

### Search images by text

```bash
export GCP_PROJECT_ID=my-project
export VECTOR_SEARCH_ENDPOINT_ID=789012
export VECTOR_SEARCH_DEPLOYED_INDEX_ID=deployed_dropbox_images

bash curl/query_vector_search.sh "a sunset over the ocean"
```

### Search documents by text

```bash
export GCP_PROJECT_ID=my-project
export VERTEX_SEARCH_DATASTORE_ID=dropbox-docs-datastore

bash curl/query_vertex_search.sh "quarterly revenue report"
```

### Combined search (images + docs)

```bash
# Set all env vars above, then:
bash curl/combine_results.sh "team meeting presentation"
```

Output is JSON:

```json
{
  "query": "team meeting presentation",
  "image_matches": [
    {"file_id": "abc123", "distance": 0.87}
  ],
  "document_matches": [
    {"document_id": "xyz", "title": "Q4 Presentation.pptx", "link": "...", "snippet": "..."}
  ]
}
```

---

## File Categories

| Category | Extensions | GCS prefix | Indexed by |
|---|---|---|---|
| images | bmp gif jpg jpeg png | `mirror/images/` | Vector Search (multimodal embeddings, dim=1408) |
| docs | pdf docx xlsx pptx txt html | `mirror/docs/` | Vertex AI Search (unstructured) |
| media | mp3 wav mp4 mov | `mirror/media/` | Stored only (no indexing) |

---

## Metadata Schema

Each file gets a JSON sidecar at `mirror/meta/<file_id>.json`:

```json
{
  "dropbox_file_id": "abc123def456",
  "dropbox_path": "/Photos/sunset.jpg",
  "rev": "015f2a...",
  "mime_type": "image/jpeg",
  "size": 2048576,
  "server_modified": "2025-12-01 10:30:00",
  "category": "images",
  "gcs_uri": "gs://my-project-dropbox-mirror/mirror/images/abc123def456",
  "caption": "sunset.jpg"
}
```

---

## Scheduling

| Job | Schedule | Purpose |
|---|---|---|
| `daily-dropbox-sync` | 02:00 UTC | Dropbox â†’ GCS incremental sync |
| `daily-image-embed` | 04:00 UTC | New/changed images â†’ Vector Search |
| `daily-docs-reimport` | 05:00 UTC | Re-import docs into Vertex AI Search |

---

## Testing

**See [TESTING.md](TESTING.md) for comprehensive test guide.**

Quick diagnostic:

```bash
# Check if Dropbox permissions are correct
bash infra/check_dropbox_permissions.sh

# Fix permissions & re-authorize
bash infra/reauthorize_dropbox.sh

# Run all tests
bash infra/run_all_tests.sh
```

Individual tests:

```bash
# Test sync
bash infra/test_sync.sh

# Test embeddings
bash infra/test_embeddings.sh

# Test document search
bash infra/test_doc_search.sh

# Test queries
bash curl/query_vector_search.sh "test query"
bash curl/query_vertex_search.sh "test query"
bash curl/combine_results.sh "test query"
```

---

## Non-Goals

This project intentionally does **NOT** include:

- RAG pipelines
- Enterprise search UI / connectors
- OCR processing
- AI captioning / tagging
- Document chunking / passage retrieval
- Pub/Sub realtime ingestion
- BigQuery indexing
- Python search backend API